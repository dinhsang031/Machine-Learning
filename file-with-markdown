 [markdown]
# ---------
# ## Giới thiệu Project
# 1. Giới thiệu Dataset và Vấn đề Cần Giải Quyết:
# 
# Dataset trên là dữ liệu về khách hàng của một công ty viễn thông, bao gồm các thông tin cá nhân và các dịch vụ mà khách hàng đang sử dụng, cũng như tình trạng Churn (khách hàng rời bỏ dịch vụ). Mục tiêu của dự án là xây dựng một mô hình Machine Learning dự báo khả năng một khách hàng sẽ Churn hay không, dựa trên các đặc điểm cá nhân và mức độ sử dụng dịch vụ của khách hàng. Việc này giúp công ty có thể dự đoán trước và áp dụng các chiến lược giảm thiểu tỷ lệ khách hàng rời bỏ, qua đó tăng doanh thu và hiệu quả dịch vụ.
# 
# 2. Giải thích Dataset:
# - CustomerID: Mã định danh của từng khách hàng.
# - Gender: Giới tính của khách hàng (Male/Female).
# - SeniorCitizen: Cho biết khách hàng có phải là người cao tuổi (SeniorCitizen = 1) hay không (SeniorCitizen = 0).
# - Partner: Tình trạng hôn nhân của khách hàng (Yes/No).
# - Dependents: Khách hàng có người phụ thuộc (Yes) hay không (No).
# - Tenure: Số tháng khách hàng đã gắn bó với công ty.
# - PhoneService: Khách hàng có sử dụng dịch vụ điện thoại hay không (Yes/No).
# - MultipleLines: Khách hàng có nhiều đường dây điện thoại không (Yes/No/No phone service).
# - InternetService: Loại dịch vụ Internet mà khách hàng sử dụng (DSL, Fiber optic, hoặc No).
# - OnlineSecurity, DeviceProtection, TechSupport, StreamingTV, StreamingMovies: Các dịch vụ bổ sung mà khách hàng có sử dụng hay không (Yes/No).
# - Contract: Loại hợp đồng của khách hàng, như hợp đồng tháng (Month-to-month), hợp đồng một năm (One year), hoặc hợp đồng hai năm (Two year).
# - PaperlessBilling: Hóa đơn điện tử (Yes) hoặc hóa đơn giấy (No).
# - PaymentMethod: Phương thức thanh toán, như chuyển khoản ngân hàng tự động, thanh toán qua séc, hoặc thanh toán qua thẻ tín dụng.
# - MonthlyCharges: Phí dịch vụ hàng tháng.
# - TotalCharges: Tổng chi phí từ trước tới nay mà khách hàng đã trả.
# - Churn: Mục tiêu cần dự đoán, xác định khách hàng có rời bỏ dịch vụ (Yes) hay không (No).
# 
# 3. Mục Tiêu Dự Án:
# 
# Mục tiêu của dự án là tìm kiếm mô hình Machine Learning phù hợp để dự báo liệu rằng khách hàng có rời bỏ dịch vụ hay không? Thiết lập bộ tham số tối ưu, lựa chọn các đặc trưng (features) có giá trị dự báo cao, và xây dựng pipeline tự động nhằm giúp công ty có thể dự báo hiệu quả khả năng khách hàng sẽ rời bỏ dịch vụ (churn) dựa trên dữ liệu hiện có. Pipeline này sẽ cho phép công ty dễ dàng áp dụng mô hình vào hệ thống Business Intelligence, hỗ trợ các chiến lược giữ chân khách hàng kịp thời và chính xác.
# 
# 4. Các bước thực hiện:
# 
# - Khám Phá và Xử Lý Dữ Liệu
# - Lựa Chọn Đặc Trưng (Feature Selection)
# - Sử dụng K-Folk đánh giá độ ổn định mô hình
# - Thiết Lập Pipeline Tự Động
# - Tìm Kiếm Mô Hình và Bộ Tham Số Tối Ưu

 [markdown]
# ## Giả sử chọn mô hình Logistics Regression:pre-procesing dataset, xây dựng mô hình

 [markdown]
# ### A Nhập thư viện và tiền xử lý dữ liệu


# Nhập thư viện
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Load dataset
df = pd.read_csv(r'c:\Users\Dell03.LAPTOP-PTNNLMOI\Desktop\SANG\Machine Learning\Midterm_K298-20241027T013250Z-001\Midterm_K298\GK_ML_K298_Nguyen_Dinh_Sang\telco_customer_churn.csv')


#5 dòng đầu dataset
df.head()


# Thông tin dataset
df.info()


# Cột TotalChages dường như có loại dữ liệu float64
df['TotalCharges']



# Chuyển đổi dữ liệu TotalChages sang kiểu số
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')


# Trình bày thông tin dataset 1 lần nữa
df.info()

 [markdown]
# => Các cột đã đúng loại dữ liệu và không có dữ liệu null


# 5 dòng đầu tiên
df.head()


# Kiểm tra các dữ liệu lỗi
df.isna().sum()

 [markdown]
# => Có dữ liệu lỗi ở cột TotalCharges, thay thế lỗi bằng trung bình của cột


# Thay thế lỗi bằng dữ liệu trung bình của feature
df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].mean())


# Kiểm tra lỗi một lần nữa
df.isna().sum()

 [markdown]
# => Dataset đã không còn lỗi


# Thống kê các cột số trong dataset
df.describe()

 [markdown]
# => Các cột số không có cùng loại đơn vị, có thể sẽ cần scale


# Kiểm tra các cột category trong dataset
columns_to_count = [
    'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'
]

# In các dữ liệu của các cột category
for col in columns_to_count:
    print(f"Value counts for {col}:\n{df[col].value_counts()}\n")


 [markdown]
# => Thay thế các giá trị "No intertnet service" và "No phone service" thành "No"


# Thay thế "No phone service" bằng "No" ở cột 'MultipleLines'
df['MultipleLines'] = df['MultipleLines'].str.replace("No phone service", "No", case=False, regex=False)

# Thay thế "No internet service" bằng "No" ở cột: 'OnlineSecurity', 'OnlineBackup' 
df['OnlineSecurity'] = df['OnlineSecurity'].str.replace("No internet service", "No", case=False, regex=False)
df['OnlineBackup'] = df['OnlineBackup'].str.replace("No internet service", "No", case=False, regex=False)
df['DeviceProtection'] = df['DeviceProtection'].str.replace("No internet service", "No", case=False, regex=False)
df['TechSupport'] = df['TechSupport'].str.replace("No internet service", "No", case=False, regex=False)
df['StreamingTV'] = df['StreamingTV'].str.replace("No internet service", "No", case=False, regex=False)
df['StreamingMovies'] = df['StreamingMovies'].str.replace("No internet service", "No", case=False, regex=False)


# Show 5 dòng đầu 1 lần nữa
df.head()


#Đếm kết quả đầu ra cần dự báo của dataset
df['Churn'].value_counts()


import seaborn as sns
# Trực quan 'Churn'
sns.countplot(data=df, x='Churn')
plt.show()


 [markdown]
# => Cho thấy sự bất cân bằng trong dữ liệu đầu ra


# Chuyển đổi output 'Churn' thành 1 hoặc 0
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})


# Chuyển đổi One-hot encoding các cột category

for column in ['Gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Contract', 'InternetService', 'PaymentMethod']:
    insert_loc = df.columns.get_loc(column)
    df = pd.concat([df.iloc[:,:insert_loc], pd.get_dummies(df.loc[:, [column]]), df.iloc[:,insert_loc+1:]], axis=1)


# Show 5 dòng dữ liệu của dataset
df.head()

 [markdown]
# ### B Tách dữ liệu và tạo model


# Chọn cột Input, không sử dụng Customer ID vì nó không hữu hiệu, và Output là Churn
# Tạo X bao gồm tất cả các cột trừ 'CustomerID' và 'Churn'
X = df.drop(columns=['CustomerID', 'Churn'])  
# Tạo y bao gồm cột 'Churn'
y = df['Churn']

# Split data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



# Show X_train
X_train.head()


from sklearn.preprocessing import StandardScaler

# Cột cần scale
scale_columns = ['Tenure', 'MonthlyCharges', 'TotalCharges']
scaler = StandardScaler()
X_train[scale_columns] = scaler.fit_transform(X_train[scale_columns])
X_test[scale_columns] = scaler.transform(X_test[scale_columns])



from sklearn.linear_model import LogisticRegression
# Học mô hình và sử dụng lớp LogisticRegression, class_weight = balanced bởi vì đầu ra không cân bằng
model = LogisticRegression(max_iter=1000, class_weight='balanced')
model.fit(X_train, y_train)


 [markdown]
# ### C Đánh giá mô hình


from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

# Đánh giá mô hình
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Precision:", precision)
print("Recall:", recall)


from sklearn.metrics import classification_report, confusion_matrix

# In classification report
print(classification_report(y_test, y_pred))



# In confusion matrix
print(confusion_matrix(y_test, y_pred))

 [markdown]
# **Kết luận:**
# - Độ chính xác tổng thể đạt 76%, cho thấy mô hình có hiệu quả khá tốt nhưng vẫn cần cải thiện.
# - Recall cao (0.84) cho lớp 1, giúp mô hình nhận diện hầu hết các trường hợp dương tính.
# - Precision thấp (0.53) cho lớp 1, có nghĩa là nhiều dự đoán nhãn dương bị sai, dẫn đến dương tính giả cao.
# - Mất cân bằng giữa precision và recall, phù hợp nếu ưu tiên phát hiện các trường hợp rủi ro (dương tính thực).
# 
# => Khuyến nghị: Có thể điều chỉnh threshold hoặc thử các mô hình khác để cải thiện hiệu suất.

 [markdown]
# ---------------------------

 [markdown]
# ## Sử dụng kỹ thuật K-fold Cross Validation để đánh giá độ ổn định của model Logistic Regression


# Sử dụng K Fold cross validation với cv=5 - tách dataset thành 5 tập validation set để kiêm tra tính ổn định
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5)

print("Cross-validation scores:", scores)
print("Mean score:", scores.mean())
print("Standard deviation:", scores.std())

 [markdown]
# Kết luận từ các thông số:
# 
# - Độ chính xác trung bình (Mean score): Mô hình có độ chính xác trung bình là khoảng 73.3% trên tập huấn luyện, cho thấy mô hình có hiệu suất ổn định.
# 
# - Độ lệch chuẩn thấp (Standard deviation): Độ lệch chuẩn là 0.0094 (tức khoảng 0.94%), chứng tỏ kết quả của các lần chạy khá đồng đều, không có sự biến động lớn giữa các lần phân chia dữ liệu khác nhau.
# 
# => Hiệu suất ổn định: Với độ lệch chuẩn thấp, mô hình thể hiện khả năng ổn định, ít bị ảnh hưởng bởi cách dữ liệu được chia trong các lần huấn luyện và kiểm tra.
# 
# => Không có hiện tượng overfitting rõ rệt: Các chỉ số cho thấy mô hình không có dấu hiệu rõ ràng của overfitting, vì độ chính xác nhất quán trong từng lần chạy.
# 
# => Tiềm năng cải thiện: Độ chính xác trung bình ở mức 73.3% cho thấy mô hình có thể cần được cải thiện thêm về tính chính xác, ví dụ thông qua việc tối ưu hóa hyperparameter hoặc xử lý thêm các đặc trưng (feature engineering).

 [markdown]
# ----------------------------------------------------

 [markdown]
# ## Lưu Logistics model và các bước Preprocesing data vào Pipeline


# Import thư viện cần thiết
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import FunctionTransformer
import pickle


 [markdown]
# ### 1.3.A Tạo Pipeline


# Bước 1: Đọc dataframe
df = pd.read_csv(r'c:\Users\Dell03.LAPTOP-PTNNLMOI\Desktop\SANG\Machine Learning\Midterm_K298-20241027T013250Z-001\Midterm_K298\GK_ML_K298_Nguyen_Dinh_Sang\telco_customer_churn.csv')


# Bước 2: Tiền xử lý dữ liệu ban đầu
def initial_preprocessing(df):
    # Convert 'TotalCharges' to numeric, replacing errors with NaN
    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
    # Fill missing values in 'TotalCharges' with mean
    df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].mean())
    
    # Replace specific values in categorical columns
    replace_columns = ['MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']
    for col in replace_columns:
        df[col] = df[col].replace({'No internet service': 'No', 'No phone service': 'No'})
    
    # Trả về DataFrame đã xử lý
    return df

# Áp dụng tiền xử lý ban đầu trên DataFrame
df = initial_preprocessing(df)


# Tách dữ liệu thành input và output
X = df.drop(columns=['CustomerID', 'Churn'])  
y = df['Churn']

# Xác định các nhóm cột
scale_columns = ['Tenure', 'MonthlyCharges', 'TotalCharges']
categorical_columns = ['Gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Contract', 'InternetService', 'PaymentMethod']



# Bước 3: Xây dựng các bước tiền xử lý trong pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), scale_columns),
        ('cat', OneHotEncoder(drop='first'), categorical_columns)
    ], remainder='passthrough'
)


# Bước 4: Xây dựng pipeline hoàn chỉnh bao gồm tiền xử lý và mô hình LogisticRegression
pipeline = Pipeline(steps=[
    ('initial_preprocessing', FunctionTransformer(initial_preprocessing)),  # Thêm bước xử lý dữ liệu ban đầu
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))
])


# Bước 5: Chia dữ liệu
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



# Bước 6: Huấn luyện pipeline trên tập huấn luyện
pipeline.fit(X_train, y_train)



# Bước 7: Lưu pipeline vào file
with open('telco_churn_pipeline.pkl', 'wb') as file:
    pickle.dump(pipeline, file)

 [markdown]
# ### 1.3.B Tải lại pipeline dự báo trên tập Test


# Tải lại pipeline từ file .pkl
with open('telco_churn_pipeline.pkl', 'rb') as file:
    pipeline = pickle.load(file)


# Dự đoán bằng pipeline đã tải
predictions = pipeline.predict(X_test)
print("Dự đoán:", predictions)

 [markdown]
# ------------------------------

 [markdown]
# ## Xác định thuật toán ML với bộ tham số tối ưu

 [markdown]
# ### A Tiền xử lý dữ liệu


# Đọc dataframe
df = pd.read_csv(r'c:\Users\Dell03.LAPTOP-PTNNLMOI\Desktop\SANG\Machine Learning\Midterm_K298-20241027T013250Z-001\Midterm_K298\GK_ML_K298_Nguyen_Dinh_Sang\telco_customer_churn.csv')


# Chuyển đổi dữ liệu TotalChages sang kiểu số
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')


# Thay thế lỗi bằng dữ liệu trung bình của feature
df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].mean())


# Thay thế "No phone service" bằng "No" ở cột 'MultipleLines'
df['MultipleLines'] = df['MultipleLines'].str.replace("No phone service", "No", case=False, regex=False)

# Thay thế "No internet service" bằng "No" ở cột: 'OnlineSecurity', 'OnlineBackup' 
df['OnlineSecurity'] = df['OnlineSecurity'].str.replace("No internet service", "No", case=False, regex=False)
df['OnlineBackup'] = df['OnlineBackup'].str.replace("No internet service", "No", case=False, regex=False)
df['DeviceProtection'] = df['DeviceProtection'].str.replace("No internet service", "No", case=False, regex=False)
df['TechSupport'] = df['TechSupport'].str.replace("No internet service", "No", case=False, regex=False)
df['StreamingTV'] = df['StreamingTV'].str.replace("No internet service", "No", case=False, regex=False)
df['StreamingMovies'] = df['StreamingMovies'].str.replace("No internet service", "No", case=False, regex=False)


# Chuyển đổi One-hot encoding các cột category

for column in ['Gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Contract', 'InternetService', 'PaymentMethod']:
    insert_loc = df.columns.get_loc(column)
    df = pd.concat([df.iloc[:,:insert_loc], pd.get_dummies(df.loc[:, [column]]), df.iloc[:,insert_loc+1:]], axis=1)


# Tạo X bao gồm tất cả các cột trừ 'CustomerID' và 'Churn'
X = df.drop(columns=['CustomerID', 'Churn'])  
# Tạo y bao gồm cột 'Churn'
y = df['Churn'].map({'Yes': 1, 'No': 0})
# Split df
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


from sklearn.preprocessing import StandardScaler

# Cột cần scale
scale_columns = ['Tenure', 'MonthlyCharges', 'TotalCharges']
scaler = StandardScaler()
X_train[scale_columns] = scaler.fit_transform(X_train[scale_columns])
X_test[scale_columns] = scaler.transform(X_test[scale_columns])


 [markdown]
# Thử các model khác nhau


from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier # KNN
from sklearn.tree import DecisionTreeClassifier # Decision Tree
from sklearn.svm import SVC # SVM
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier # Random Forest, AdaBoost
import xgboost as xgb # XGBoost
from sklearn.model_selection import GridSearchCV
import pandas as pd
import time
import warnings
warnings.filterwarnings('ignore') 


# List the model
models = [KNeighborsClassifier(), RandomForestClassifier(), DecisionTreeClassifier(), SVC(), AdaBoostClassifier(),  xgb.XGBClassifier()]
scores = []
train_times = []
names = []

for model in models:
    start = time.time()
    scores.append(cross_val_score(estimator = model, X = X_train, y = y_train, scoring="accuracy", cv=5).mean())
    end = time.time()
    train_times.append(end-start)
    names.append(model.__class__.__name__)

df = pd.DataFrame(scores, columns=['score'], index=range(len(models)))
df.insert(1,'Time', pd.Series(train_times))
df.insert(0,'Model', pd.Series(names))
df.head(10)

 [markdown]
# Ở đây chưa chọn được dựa trên score và time và cần sử dụng phương pháp Grid Search (Chạy các bộ tham số) vì các bộ tham số có thể dẫn đến việc tối ưu của các mô hình

 [markdown]
# ### B Chạy vòng lặp với các bộ tham số cho từng model


# Danh sách các mô hình và tham số
model_params = [
    {
        'name': 'KNN',
        'model': KNeighborsClassifier(),
        'params': {
            'n_neighbors': [3, 5, 7, 9, 11, 13, 15],
            'weights': ['uniform', 'distance'],
            'metric': ['minkowski', 'euclidean', 'manhattan']
        }
    },
    {
        'name': 'Decision Tree',
        'model': DecisionTreeClassifier(),
        'params': {
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 5, 10],
            'max_features': ['auto', 'sqrt', 'log2']
        }
    },
    {
        'name': 'Random Forest',
        'model': RandomForestClassifier(n_jobs=-1),
        'params': {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['auto', 'sqrt', 'log2']
        }
    },
    {
        'name': 'AdaBoost',
        'model': AdaBoostClassifier(),
        'params': {
            'n_estimators': [100, 150, 200],
            'learning_rate': [0.01, 0.1, 1.0],
            'algorithm': ['SAMME', 'SAMME.R']
        }
    },
    {
        'name': 'XGBoost',
        'model': xgb.XGBClassifier(),
        'params': {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7],
            'subsample': [0.8, 1.0],
            'colsample_bytree': [0.8, 1.0]
        }
    }
]

# Danh sách lưu trữ kết quả
results = []

# Vòng lặp qua các mô hình và thực hiện GridSearchCV
for mp in model_params:
    model_name = mp['name']
    model = mp['model']
    params = mp['params']
    
    # Thời gian bắt đầu
    start_time = time.time()
    
    # GridSearchCV
    grid_search = GridSearchCV(model, params, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    
    # Thời gian kết thúc
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # Lưu kết quả
    results.append({
        'Model': model_name,
        'Best Parameters': grid_search.best_params_,
        'Best Score': grid_search.best_score_,
        'Time': elapsed_time
    })

# Chuyển kết quả thành DataFrame
df_results = pd.DataFrame(results)
print(df_results)


# Cấu hình để in đầy đủ nội dung trong các cột
pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_columns', None)
# Lọc DataFrame cho 3 mô hình đã kết luận: XGBoost, AdaBoost, và Random Forest
selected_models = df_results[df_results['Model'].isin(['XGBoost', 'AdaBoost'])]

# In kết quả của 3 mô hình
print("Các model có score và hiệu suất cao nhất: ", selected_models)


 [markdown]
# Kết luận
# - Lựa chọn ưu tiên: XGBoost (cao nhất về độ chính xác), trong trường hợp có thể chấp nhận thời gian huấn luyện lâu.
# - Thay thế: AdaBoost (trong trường hợp muốn cân bằng độ chính xác và thời gian).
# 
# => XGBoost có vẻ là lựa chọn tối ưu nhất dựa trên độ chính xác và thời gian xử lý.
# 

 [markdown]
# ------------------
# 

 [markdown]
# ## Chọn thuật toán Model XGBoost, tìm 10 features quan trọng để thực hiện tối ưu model


# Import thư viện
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
import pandas as pd
import matplotlib.pyplot as plt


# Load và tiền xử lý dữ liệu
df = pd.read_csv(r'c:\Users\Dell03.LAPTOP-PTNNLMOI\Desktop\SANG\Machine Learning\Midterm_K298-20241027T013250Z-001\Midterm_K298\GK_ML_K298_Nguyen_Dinh_Sang\telco_customer_churn.csv')

# Tiền xử lý dữ liệu ban đầu bằng cách gọi hàm đã khai báo ở 1.3 Tạo Pipeline
initial_preprocessing(df)

# Tách input và output
X = df.drop(columns=['CustomerID', 'Churn'])
y = df['Churn']



# Định nghĩa các cột để scale và one-hot encode
scale_columns = ['Tenure', 'MonthlyCharges', 'TotalCharges']
categorical_columns = [
    'Gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',
    'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Contract',
    'InternetService', 'PaymentMethod'
]

# Cấu hình tiền xử lý
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), scale_columns),
        ('cat', OneHotEncoder(drop='first'), categorical_columns)
    ], remainder='passthrough'
)

# Áp dụng tiền xử lý cho X
X_processed = preprocessor.fit_transform(X)



from sklearn.preprocessing import LabelEncoder

# Chuyển đổi nhãn y từ 'No' và 'Yes' sang 0 và 1
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Chia dữ liệu thành tập huấn luyện và tập kiểm tra
X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.3, random_state=42)

# Huấn luyện model XGBoost với các tham số tối ưu
xgb_model = xgb.XGBClassifier(
    colsample_bytree=1.0, 
    learning_rate=0.1, 
    max_depth=3, 
    n_estimators=100, 
    subsample=1.0
)
xgb_model.fit(X_train, y_train)


# Lấy các độ quan trọng của các feature
importance = xgb_model.feature_importances_

# Chuyển thành DataFrame và sắp xếp để lấy top 10
features = preprocessor.get_feature_names_out()
importance_df = pd.DataFrame({'Feature': features, 'Importance': importance})
top_10_features = importance_df.sort_values(by='Importance', ascending=False).head(10)
print("Top 10 features quan trọng:")
print(top_10_features)

# Vẽ biểu đồ cho 10 feature quan trọng nhất
plt.figure(figsize=(10, 6))
plt.barh(top_10_features['Feature'], top_10_features['Importance'], color='skyblue')
plt.xlabel("Importance")
plt.title("Top 10 features quan trọng cho XGBoost Model")
plt.gca().invert_yaxis()
plt.show()


 [markdown]
# Từ top 10 features, có thể thấy rằng các yếu tố về loại dịch vụ Internet (đặc biệt là Fiber optic) và thời hạn hợp đồng (hai năm, một năm) có tác động lớn nhất đến Kết quả Churn, cho thấy rằng chất lượng dịch vụ và cam kết dài hạn có ảnh hưởng mạnh mẽ đến quyết định của khách hàng. Những yếu tố khác như dịch vụ hỗ trợ (PhoneService, TechSupport) và tiện ích bổ sung (StreamingMovies, OnlineSecurity) đóng vai trò quan trọng nhưng ít hơn, gợi ý rằng sự hài lòng với các dịch vụ kèm theo cũng góp phần giữ chân khách hàng.


from sklearn.metrics import accuracy_score, classification_report

# Lấy danh sách các cột top 10 features
top_10_feature_names = top_10_features['Feature'].values

# Lọc lại dữ liệu chỉ với 10 feature quan trọng nhất
X_top_10 = X_processed[:, [features.tolist().index(f) for f in top_10_feature_names]]

# Chia dữ liệu mới thành tập huấn luyện và tập kiểm tra
X_train_top_10, X_test_top_10, y_train_top_10, y_test_top_10 = train_test_split(X_top_10, y_encoded, test_size=0.3, random_state=42)

# Huấn luyện model XGBoost với 10 feature quan trọng nhất

scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) # Xử lý sự mất cân bằng của output

xgb_model_top_10 = XGBClassifier(
    scale_pos_weight=scale_pos_weight, # Xử lý sự mất cân bằng của output
    colsample_bytree=1.0, 
    learning_rate=0.1, 
    max_depth=3, 
    n_estimators=100, 
    subsample=1.0
)
xgb_model_top_10.fit(X_train_top_10, y_train_top_10)

# Dự đoán trên tập kiểm tra
y_pred_top_10 = xgb_model_top_10.predict(X_test_top_10)

# Đánh giá model
accuracy = accuracy_score(y_test_top_10, y_pred_top_10)
report = classification_report(y_test_top_10, y_pred_top_10)

print(f"Accuracy của model với 10 feature quan trọng nhất: {accuracy}")
print("Classification Report:")
print(report)

 [markdown]
# Dựa vào Accuracy và Classification Report, có một số nhận xét sau:
# 
# Model đạt được 75.14% độ chính xác, cho thấy hiệu quả tương đối tốt trong việc phân loại với chỉ 10 feature quan trọng nhất.
# Tuy nhiên, độ chính xác này có phần thấp hơn so với các model thường sử dụng toàn bộ feature.
# 
# - Class 0 (Không Churn) có độ chính xác cao hơn (92%), cho thấy model tốt trong việc nhận diện các khách hàng sẽ không rời bỏ dịch vụ.
# - Class 1 (Churn) có độ chính xác chỉ đạt 53%, điều này nghĩa là model dễ nhầm các khách hàng có khả năng rời bỏ dịch vụ (Churn) với các khách hàng không rời bỏ.
# 
# Kết luận:
# - Model này đạt hiệu quả tốt trong việc dự đoán khách hàng không rời bỏ, nhưng vẫn gặp hạn chế khi phát hiện các khách hàng có khả năng rời bỏ dịch vụ.
# - Có thể sử dụng thêm nhiều feature hoặc tối ưu lại các feature được lựa chọn.


